import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Linear Discriminant Analysis.

  A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.

  The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.

  The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the `transform` method.

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)
 */
export declare class LinearDiscriminantAnalysis {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          ‘svd’: Singular value decomposition (default). Does not compute the covariance matrix, therefore this solver is recommended for data with a large number of features.
    
          @defaultValue `'svd'`
         */
        solver?: 'svd' | 'lsqr' | 'eigen';
        /**
          None: no shrinkage (default).
         */
        shrinkage?: 'auto' | number;
        /**
          The class prior probabilities. By default, the class proportions are inferred from the training data.
         */
        priors?: ArrayLike;
        /**
          Number of components (<= min(n\_classes - 1, n\_features)) for dimensionality reduction. If `undefined`, will be set to min(n\_classes - 1, n\_features). This parameter only affects the `transform` method.
         */
        n_components?: number;
        /**
          If `true`, explicitly compute the weighted within-class covariance matrix when solver is ‘svd’. The matrix is always computed and stored for the other solvers.
    
          @defaultValue `false`
         */
        store_covariance?: boolean;
        /**
          Absolute threshold for a singular value of X to be considered significant, used to estimate the rank of X. Dimensions whose singular values are non-significant are discarded. Only used if solver is ‘svd’.
    
          @defaultValue `0.0001`
         */
        tol?: number;
        /**
          If not `undefined`, `covariance\_estimator` is used to estimate the covariance matrices instead of relying on the empirical covariance estimator (with potential shrinkage). The object should have a fit method and a `covariance\_` attribute like the estimators in [`sklearn.covariance`](../classes.html#module-sklearn.covariance "sklearn.covariance"). if `undefined` the shrinkage parameter drives the estimate.
    
          This should be left to `undefined` if `shrinkage` is used. Note that `covariance\_estimator` works only with ‘lsqr’ and ‘eigen’ solvers.
         */
        covariance_estimator?: any;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Apply decision function to an array of samples.
  
      The decision function is equal (up to a constant factor) to the log-posterior of the model, i.e. `log p(y \= k | x)`. In a binary classification setting this instead corresponds to the difference `log p(y \= 1 | x) \- log p(y \= 0 | x)`. See [Mathematical formulation of the LDA and QDA classifiers](../lda_qda.html#lda-qda-math).
     */
    decision_function(opts: {
        /**
          Array of samples (test vectors).
         */
        X?: ArrayLike[];
    }): Promise<NDArray>;
    /**
      Fit the Linear Discriminant Analysis model.
     */
    fit(opts: {
        /**
          Training data.
         */
        X?: ArrayLike[];
        /**
          Target values.
         */
        y?: ArrayLike;
    }): Promise<any>;
    /**
      Fit to data, then transform it.
  
      Fits transformer to `X` and `y` with optional parameters `fit\_params` and returns a transformed version of `X`.
     */
    fit_transform(opts: {
        /**
          Input samples.
         */
        X?: ArrayLike[];
        /**
          Target values (`undefined` for unsupervised transformations).
         */
        y?: ArrayLike;
        /**
          Additional fit parameters.
         */
        fit_params?: any;
    }): Promise<any[]>;
    /**
      Get output feature names for transformation.
  
      The feature names out will prefixed by the lowercased class name. For example, if the transformer outputs 3 features, then the feature names out are: `\["class\_name0", "class\_name1", "class\_name2"\]`.
     */
    get_feature_names_out(opts: {
        /**
          Only used to validate feature names with the names seen in `fit`.
         */
        input_features?: any;
    }): Promise<any>;
    /**
      Get metadata routing of this object.
  
      Please check [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.
     */
    get_metadata_routing(opts: {
        /**
          A [`MetadataRequest`](sklearn.utils.metadata_routing.MetadataRequest.html#sklearn.utils.metadata_routing.MetadataRequest "sklearn.utils.metadata_routing.MetadataRequest") encapsulating routing information.
         */
        routing?: any;
    }): Promise<any>;
    /**
      Predict class labels for samples in X.
     */
    predict(opts: {
        /**
          The data matrix for which we want to get the predictions.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray>;
    /**
      Estimate log probability.
     */
    predict_log_proba(opts: {
        /**
          Input data.
         */
        X?: ArrayLike[];
    }): Promise<NDArray[]>;
    /**
      Estimate probability.
     */
    predict_proba(opts: {
        /**
          Input data.
         */
        X?: ArrayLike[];
    }): Promise<NDArray[]>;
    /**
      Return the mean accuracy on the given test data and labels.
  
      In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.
     */
    score(opts: {
        /**
          Test samples.
         */
        X?: ArrayLike[];
        /**
          True labels for `X`.
         */
        y?: ArrayLike;
        /**
          Sample weights.
         */
        sample_weight?: ArrayLike;
    }): Promise<number>;
    /**
      Set output container.
  
      See [Introducing the set\_output API](../../auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py) for an example on how to use the API.
     */
    set_output(opts: {
        /**
          Configure output of `transform` and `fit\_transform`.
         */
        transform?: 'default' | 'pandas';
    }): Promise<any>;
    /**
      Request metadata passed to the `score` method.
  
      Note that this method is only relevant if `enable\_metadata\_routing=True` (see [`sklearn.set\_config`](sklearn.set_config.html#sklearn.set_config "sklearn.set_config")). Please see [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.
  
      The options for each parameter are:
     */
    set_score_request(opts: {
        /**
          Metadata routing for `sample\_weight` parameter in `score`.
         */
        sample_weight?: string | boolean;
    }): Promise<any>;
    /**
      Project data to maximize class separation.
     */
    transform(opts: {
        /**
          Input data.
         */
        X?: ArrayLike[];
    }): Promise<NDArray[]>;
    /**
      Weight vector(s).
     */
    get coef_(): Promise<NDArray>;
    /**
      Intercept term.
     */
    get intercept_(): Promise<NDArray>;
    /**
      Weighted within-class covariance matrix. It corresponds to `sum\_k prior\_k \* C\_k` where `C\_k` is the covariance matrix of the samples in class `k`. The `C\_k` are estimated using the (potentially shrunk) biased estimator of covariance. If solver is ‘svd’, only exists when `store\_covariance` is `true`.
     */
    get covariance_(): Promise<ArrayLike[]>;
    /**
      Percentage of variance explained by each of the selected components. If `n\_components` is not set then all components are stored and the sum of explained variances is equal to 1.0. Only available when eigen or svd solver is used.
     */
    get explained_variance_ratio_(): Promise<NDArray>;
    /**
      Class-wise means.
     */
    get means_(): Promise<ArrayLike[]>;
    /**
      Class priors (sum to 1).
     */
    get priors_(): Promise<ArrayLike>;
    /**
      Scaling of the features in the space spanned by the class centroids. Only available for ‘svd’ and ‘eigen’ solvers.
     */
    get scalings_(): Promise<ArrayLike[]>;
    /**
      Overall mean. Only present if solver is ‘svd’.
     */
    get xbar_(): Promise<ArrayLike>;
    /**
      Unique class labels.
     */
    get classes_(): Promise<ArrayLike>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
//# sourceMappingURL=LinearDiscriminantAnalysis.d.ts.map