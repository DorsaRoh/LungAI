{"version":3,"sources":["../../../../src/generated/feature_extraction/text/TfidfVectorizer.ts"],"sourcesContent":["/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  Convert a collection of raw documents to a matrix of TF-IDF features.\n\n  Equivalent to [`CountVectorizer`](sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer \"sklearn.feature_extraction.text.CountVectorizer\") followed by [`TfidfTransformer`](sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer \"sklearn.feature_extraction.text.TfidfTransformer\").\n\n  For an example of usage, see [Classification of text documents using sparse features](../../auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py).\n\n  For an efficiency comparision of the different feature extractors, see [FeatureHasher and DictVectorizer Comparison](../../auto_examples/text/plot_hashing_vs_dict_vectorizer.html#sphx-glr-auto-examples-text-plot-hashing-vs-dict-vectorizer-py).\n\n  Read more in the [User Guide](../feature_extraction.html#text-feature-extraction).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n */\nexport class TfidfVectorizer {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      If `'filename'`, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.\n\n      @defaultValue `'content'`\n     */\n    input?: 'filename' | 'file' | 'content'\n\n    /**\n      If bytes or files are given to analyze, this encoding is used to decode.\n\n      @defaultValue `'utf-8'`\n     */\n    encoding?: string\n\n    /**\n      Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is ‘strict’, meaning that a UnicodeDecodeError will be raised. Other values are ‘ignore’ and ‘replace’.\n\n      @defaultValue `'strict'`\n     */\n    decode_error?: 'strict' | 'ignore' | 'replace'\n\n    /**\n      Remove accents and perform other character normalization during the preprocessing step. ‘ascii’ is a fast method that only works on characters that have a direct ASCII mapping. ‘unicode’ is a slightly slower method that works on any characters. `undefined` (default) means no character normalization is performed.\n\n      Both ‘ascii’ and ‘unicode’ use NFKD normalization from [`unicodedata.normalize`](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize \"(in Python v3.12)\").\n     */\n    strip_accents?: 'ascii' | 'unicode'\n\n    /**\n      Convert all characters to lowercase before tokenizing.\n\n      @defaultValue `true`\n     */\n    lowercase?: boolean\n\n    /**\n      Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if `analyzer` is not callable.\n     */\n    preprocessor?: any\n\n    /**\n      Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if `analyzer \\== 'word'`.\n     */\n    tokenizer?: any\n\n    /**\n      Whether the feature should be made of word or character n-grams. Option ‘char\\_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n\n      If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.\n\n      @defaultValue `'word'`\n     */\n    analyzer?: 'word' | 'char' | 'char_wb'\n\n    /**\n      If a string, it is passed to \\_check\\_stop\\_list and the appropriate stop list is returned. ‘english’ is currently the only supported string value. There are several known issues with ‘english’ and you should consider an alternative (see [Using stop words](../feature_extraction.html#stop-words)).\n\n      If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if `analyzer \\== 'word'`.\n\n      If `undefined`, no stop words will be used. In this case, setting `max\\_df` to a higher value, such as in the range (0.7, 1.0), can automatically detect and filter stop words based on intra corpus document frequency of terms.\n     */\n    stop_words?: 'english' | any[]\n\n    /**\n      Regular expression denoting what constitutes a “token”, only used if `analyzer \\== 'word'`. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n\n      If there is a capturing group in token\\_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted.\n     */\n    token_pattern?: string\n\n    /**\n      The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min\\_n <= n <= max\\_n will be used. For example an `ngram\\_range` of `(1, 1)` means only unigrams, `(1, 2)` means unigrams and bigrams, and `(2, 2)` means only bigrams. Only applies if `analyzer` is not callable.\n     */\n    ngram_range?: any\n\n    /**\n      When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range \\[0.0, 1.0\\], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not `undefined`.\n\n      @defaultValue `1`\n     */\n    max_df?: number\n\n    /**\n      When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float in range of \\[0.0, 1.0\\], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not `undefined`.\n\n      @defaultValue `1`\n     */\n    min_df?: number\n\n    /**\n      If not `undefined`, build a vocabulary that only consider the top `max\\_features` ordered by term frequency across the corpus. Otherwise, all features are used.\n\n      This parameter is ignored if vocabulary is not `undefined`.\n     */\n    max_features?: number\n\n    /**\n      Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents.\n     */\n    vocabulary?: any\n\n    /**\n      If `true`, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set `binary` to `true`, `use\\_idf` to `false` and `norm` to `undefined` to get 0/1 outputs).\n\n      @defaultValue `false`\n     */\n    binary?: boolean\n\n    /**\n      Type of the matrix returned by fit\\_transform() or transform().\n     */\n    dtype?: any\n\n    /**\n      Each output row will have unit norm, either:\n\n      @defaultValue `'l2'`\n     */\n    norm?: 'l1' | 'l2'\n\n    /**\n      Enable inverse-document-frequency reweighting. If `false`, idf(t) = 1.\n\n      @defaultValue `true`\n     */\n    use_idf?: boolean\n\n    /**\n      Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n\n      @defaultValue `true`\n     */\n    smooth_idf?: boolean\n\n    /**\n      Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\n      @defaultValue `false`\n     */\n    sublinear_tf?: boolean\n  }) {\n    this.id = `TfidfVectorizer${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error('TfidfVectorizer.init requires a PythonBridge instance')\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntry: bridgeTfidfVectorizer\nexcept NameError: bridgeTfidfVectorizer = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_TfidfVectorizer = {'input': ${\n      this.opts['input'] ?? undefined\n    }, 'encoding': ${this.opts['encoding'] ?? undefined}, 'decode_error': ${\n      this.opts['decode_error'] ?? undefined\n    }, 'strip_accents': ${\n      this.opts['strip_accents'] ?? undefined\n    }, 'lowercase': ${this.opts['lowercase'] ?? undefined}, 'preprocessor': ${\n      this.opts['preprocessor'] ?? undefined\n    }, 'tokenizer': ${this.opts['tokenizer'] ?? undefined}, 'analyzer': ${\n      this.opts['analyzer'] ?? undefined\n    }, 'stop_words': ${\n      this.opts['stop_words'] ?? undefined\n    }, 'token_pattern': ${\n      this.opts['token_pattern'] ?? undefined\n    }, 'ngram_range': ${this.opts['ngram_range'] ?? undefined}, 'max_df': ${\n      this.opts['max_df'] ?? undefined\n    }, 'min_df': ${this.opts['min_df'] ?? undefined}, 'max_features': ${\n      this.opts['max_features'] ?? undefined\n    }, 'vocabulary': ${this.opts['vocabulary'] ?? undefined}, 'binary': ${\n      this.opts['binary'] ?? undefined\n    }, 'dtype': ${this.opts['dtype'] ?? undefined}, 'norm': ${\n      this.opts['norm'] ?? undefined\n    }, 'use_idf': ${this.opts['use_idf'] ?? undefined}, 'smooth_idf': ${\n      this.opts['smooth_idf'] ?? undefined\n    }, 'sublinear_tf': ${this.opts['sublinear_tf'] ?? undefined}}\n\nctor_TfidfVectorizer = {k: v for k, v in ctor_TfidfVectorizer.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeTfidfVectorizer[${this.id}] = TfidfVectorizer(**ctor_TfidfVectorizer)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeTfidfVectorizer[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Return a callable to process input data.\n\n    The callable handles preprocessing, tokenization, and n-grams generation.\n   */\n  async build_analyzer(opts: {}): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'TfidfVectorizer must call init() before build_analyzer()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_TfidfVectorizer_build_analyzer = {}\n\npms_TfidfVectorizer_build_analyzer = {k: v for k, v in pms_TfidfVectorizer_build_analyzer.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_build_analyzer = bridgeTfidfVectorizer[${this.id}].build_analyzer(**pms_TfidfVectorizer_build_analyzer)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_build_analyzer.tolist() if hasattr(res_TfidfVectorizer_build_analyzer, 'tolist') else res_TfidfVectorizer_build_analyzer`\n  }\n\n  /**\n    Return a function to preprocess the text before tokenization.\n   */\n  async build_preprocessor(opts: {}): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'TfidfVectorizer must call init() before build_preprocessor()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_TfidfVectorizer_build_preprocessor = {}\n\npms_TfidfVectorizer_build_preprocessor = {k: v for k, v in pms_TfidfVectorizer_build_preprocessor.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_build_preprocessor = bridgeTfidfVectorizer[${this.id}].build_preprocessor(**pms_TfidfVectorizer_build_preprocessor)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_build_preprocessor.tolist() if hasattr(res_TfidfVectorizer_build_preprocessor, 'tolist') else res_TfidfVectorizer_build_preprocessor`\n  }\n\n  /**\n    Return a function that splits a string into a sequence of tokens.\n   */\n  async build_tokenizer(opts: {}): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'TfidfVectorizer must call init() before build_tokenizer()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_TfidfVectorizer_build_tokenizer = {}\n\npms_TfidfVectorizer_build_tokenizer = {k: v for k, v in pms_TfidfVectorizer_build_tokenizer.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_build_tokenizer = bridgeTfidfVectorizer[${this.id}].build_tokenizer(**pms_TfidfVectorizer_build_tokenizer)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_build_tokenizer.tolist() if hasattr(res_TfidfVectorizer_build_tokenizer, 'tolist') else res_TfidfVectorizer_build_tokenizer`\n  }\n\n  /**\n    Decode the input into a string of unicode symbols.\n\n    The decoding strategy depends on the vectorizer parameters.\n   */\n  async decode(opts: {\n    /**\n      The string to decode.\n     */\n    doc?: string\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('TfidfVectorizer must call init() before decode()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_TfidfVectorizer_decode = {'doc': ${\n      opts['doc'] ?? undefined\n    }}\n\npms_TfidfVectorizer_decode = {k: v for k, v in pms_TfidfVectorizer_decode.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_decode = bridgeTfidfVectorizer[${this.id}].decode(**pms_TfidfVectorizer_decode)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_decode.tolist() if hasattr(res_TfidfVectorizer_decode, 'tolist') else res_TfidfVectorizer_decode`\n  }\n\n  /**\n    Learn vocabulary and idf from training set.\n   */\n  async fit(opts: {\n    /**\n      An iterable which generates either str, unicode or file objects.\n     */\n    raw_documents?: any\n\n    /**\n      This parameter is not needed to compute tfidf.\n     */\n    y?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('TfidfVectorizer must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_TfidfVectorizer_fit = {'raw_documents': ${\n      opts['raw_documents'] ?? undefined\n    }, 'y': ${opts['y'] ?? undefined}}\n\npms_TfidfVectorizer_fit = {k: v for k, v in pms_TfidfVectorizer_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_fit = bridgeTfidfVectorizer[${this.id}].fit(**pms_TfidfVectorizer_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_fit.tolist() if hasattr(res_TfidfVectorizer_fit, 'tolist') else res_TfidfVectorizer_fit`\n  }\n\n  /**\n    Learn vocabulary and idf, return document-term matrix.\n\n    This is equivalent to fit followed by transform, but more efficiently implemented.\n   */\n  async fit_transform(opts: {\n    /**\n      An iterable which generates either str, unicode or file objects.\n     */\n    raw_documents?: any\n\n    /**\n      This parameter is ignored.\n     */\n    y?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('TfidfVectorizer must call init() before fit_transform()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_TfidfVectorizer_fit_transform = {'raw_documents': ${\n      opts['raw_documents'] ?? undefined\n    }, 'y': ${opts['y'] ?? undefined}}\n\npms_TfidfVectorizer_fit_transform = {k: v for k, v in pms_TfidfVectorizer_fit_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_fit_transform = bridgeTfidfVectorizer[${this.id}].fit_transform(**pms_TfidfVectorizer_fit_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_fit_transform.tolist() if hasattr(res_TfidfVectorizer_fit_transform, 'tolist') else res_TfidfVectorizer_fit_transform`\n  }\n\n  /**\n    Get output feature names for transformation.\n   */\n  async get_feature_names_out(opts: {\n    /**\n      Not used, present here for API consistency by convention.\n     */\n    input_features?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'TfidfVectorizer must call init() before get_feature_names_out()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_TfidfVectorizer_get_feature_names_out = {'input_features': ${\n      opts['input_features'] ?? undefined\n    }}\n\npms_TfidfVectorizer_get_feature_names_out = {k: v for k, v in pms_TfidfVectorizer_get_feature_names_out.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_get_feature_names_out = bridgeTfidfVectorizer[${this.id}].get_feature_names_out(**pms_TfidfVectorizer_get_feature_names_out)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_get_feature_names_out.tolist() if hasattr(res_TfidfVectorizer_get_feature_names_out, 'tolist') else res_TfidfVectorizer_get_feature_names_out`\n  }\n\n  /**\n    Get metadata routing of this object.\n\n    Please check [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.\n   */\n  async get_metadata_routing(opts: {\n    /**\n      A [`MetadataRequest`](sklearn.utils.metadata_routing.MetadataRequest.html#sklearn.utils.metadata_routing.MetadataRequest \"sklearn.utils.metadata_routing.MetadataRequest\") encapsulating routing information.\n     */\n    routing?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'TfidfVectorizer must call init() before get_metadata_routing()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_TfidfVectorizer_get_metadata_routing = {'routing': ${\n      opts['routing'] ?? undefined\n    }}\n\npms_TfidfVectorizer_get_metadata_routing = {k: v for k, v in pms_TfidfVectorizer_get_metadata_routing.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_get_metadata_routing = bridgeTfidfVectorizer[${this.id}].get_metadata_routing(**pms_TfidfVectorizer_get_metadata_routing)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_get_metadata_routing.tolist() if hasattr(res_TfidfVectorizer_get_metadata_routing, 'tolist') else res_TfidfVectorizer_get_metadata_routing`\n  }\n\n  /**\n    Build or fetch the effective stop words list.\n   */\n  async get_stop_words(opts: {}): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'TfidfVectorizer must call init() before get_stop_words()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_TfidfVectorizer_get_stop_words = {}\n\npms_TfidfVectorizer_get_stop_words = {k: v for k, v in pms_TfidfVectorizer_get_stop_words.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_get_stop_words = bridgeTfidfVectorizer[${this.id}].get_stop_words(**pms_TfidfVectorizer_get_stop_words)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_get_stop_words.tolist() if hasattr(res_TfidfVectorizer_get_stop_words, 'tolist') else res_TfidfVectorizer_get_stop_words`\n  }\n\n  /**\n    Return terms per document with nonzero entries in X.\n   */\n  async inverse_transform(opts: {\n    /**\n      Document-term matrix.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'TfidfVectorizer must call init() before inverse_transform()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_TfidfVectorizer_inverse_transform = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_TfidfVectorizer_inverse_transform = {k: v for k, v in pms_TfidfVectorizer_inverse_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_inverse_transform = bridgeTfidfVectorizer[${this.id}].inverse_transform(**pms_TfidfVectorizer_inverse_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_inverse_transform.tolist() if hasattr(res_TfidfVectorizer_inverse_transform, 'tolist') else res_TfidfVectorizer_inverse_transform`\n  }\n\n  /**\n    Request metadata passed to the `fit` method.\n\n    Note that this method is only relevant if `enable\\_metadata\\_routing=True` (see [`sklearn.set\\_config`](sklearn.set_config.html#sklearn.set_config \"sklearn.set_config\")). Please see [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.\n\n    The options for each parameter are:\n   */\n  async set_fit_request(opts: {\n    /**\n      Metadata routing for `raw\\_documents` parameter in `fit`.\n     */\n    raw_documents?: string | boolean\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'TfidfVectorizer must call init() before set_fit_request()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_TfidfVectorizer_set_fit_request = {'raw_documents': ${\n      opts['raw_documents'] ?? undefined\n    }}\n\npms_TfidfVectorizer_set_fit_request = {k: v for k, v in pms_TfidfVectorizer_set_fit_request.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_set_fit_request = bridgeTfidfVectorizer[${this.id}].set_fit_request(**pms_TfidfVectorizer_set_fit_request)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_set_fit_request.tolist() if hasattr(res_TfidfVectorizer_set_fit_request, 'tolist') else res_TfidfVectorizer_set_fit_request`\n  }\n\n  /**\n    Request metadata passed to the `transform` method.\n\n    Note that this method is only relevant if `enable\\_metadata\\_routing=True` (see [`sklearn.set\\_config`](sklearn.set_config.html#sklearn.set_config \"sklearn.set_config\")). Please see [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.\n\n    The options for each parameter are:\n   */\n  async set_transform_request(opts: {\n    /**\n      Metadata routing for `raw\\_documents` parameter in `transform`.\n     */\n    raw_documents?: string | boolean\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'TfidfVectorizer must call init() before set_transform_request()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_TfidfVectorizer_set_transform_request = {'raw_documents': ${\n      opts['raw_documents'] ?? undefined\n    }}\n\npms_TfidfVectorizer_set_transform_request = {k: v for k, v in pms_TfidfVectorizer_set_transform_request.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_set_transform_request = bridgeTfidfVectorizer[${this.id}].set_transform_request(**pms_TfidfVectorizer_set_transform_request)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_set_transform_request.tolist() if hasattr(res_TfidfVectorizer_set_transform_request, 'tolist') else res_TfidfVectorizer_set_transform_request`\n  }\n\n  /**\n    Transform documents to document-term matrix.\n\n    Uses the vocabulary and document frequencies (df) learned by fit (or fit\\_transform).\n   */\n  async transform(opts: {\n    /**\n      An iterable which generates either str, unicode or file objects.\n     */\n    raw_documents?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('TfidfVectorizer must call init() before transform()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_TfidfVectorizer_transform = {'raw_documents': ${\n      opts['raw_documents'] ?? undefined\n    }}\n\npms_TfidfVectorizer_transform = {k: v for k, v in pms_TfidfVectorizer_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_TfidfVectorizer_transform = bridgeTfidfVectorizer[${this.id}].transform(**pms_TfidfVectorizer_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_TfidfVectorizer_transform.tolist() if hasattr(res_TfidfVectorizer_transform, 'tolist') else res_TfidfVectorizer_transform`\n  }\n\n  /**\n    A mapping of terms to feature indices.\n   */\n  get vocabulary_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'TfidfVectorizer must call init() before accessing vocabulary_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_TfidfVectorizer_vocabulary_ = bridgeTfidfVectorizer[${this.id}].vocabulary_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_TfidfVectorizer_vocabulary_.tolist() if hasattr(attr_TfidfVectorizer_vocabulary_, 'tolist') else attr_TfidfVectorizer_vocabulary_`\n    })()\n  }\n\n  /**\n    True if a fixed vocabulary of term to indices mapping is provided by the user.\n   */\n  get fixed_vocabulary_(): Promise<boolean> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'TfidfVectorizer must call init() before accessing fixed_vocabulary_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_TfidfVectorizer_fixed_vocabulary_ = bridgeTfidfVectorizer[${this.id}].fixed_vocabulary_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_TfidfVectorizer_fixed_vocabulary_.tolist() if hasattr(attr_TfidfVectorizer_fixed_vocabulary_, 'tolist') else attr_TfidfVectorizer_fixed_vocabulary_`\n    })()\n  }\n\n  /**\n    Terms that were ignored because they either:\n   */\n  get stop_words_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This TfidfVectorizer instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'TfidfVectorizer must call init() before accessing stop_words_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_TfidfVectorizer_stop_words_ = bridgeTfidfVectorizer[${this.id}].stop_words_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_TfidfVectorizer_stop_words_.tolist() if hasattr(attr_TfidfVectorizer_stop_words_, 'tolist') else attr_TfidfVectorizer_stop_words_`\n    })()\n  }\n}\n"],"mappings":";AAGA,OAAO,YAAY;AAiBZ,IAAM,kBAAN,MAAsB;AAAA,EAQ3B,YAAY,MA6IT;AAhJH,0BAA0B;AAC1B,uBAAuB;AAgJrB,SAAK,KAAK,kBAAkB,OAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAC5D,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI,MAAM,uDAAuD;AAAA,IACzE;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,sCACb,KAAK,KAAK,OAAO,KAAK,uBACP,KAAK,KAAK,UAAU,KAAK,2BACxC,KAAK,KAAK,cAAc,KAAK,4BAE7B,KAAK,KAAK,eAAe,KAAK,wBACd,KAAK,KAAK,WAAW,KAAK,2BAC1C,KAAK,KAAK,cAAc,KAAK,wBACb,KAAK,KAAK,WAAW,KAAK,uBAC1C,KAAK,KAAK,UAAU,KAAK,yBAEzB,KAAK,KAAK,YAAY,KAAK,4BAE3B,KAAK,KAAK,eAAe,KAAK,0BACZ,KAAK,KAAK,aAAa,KAAK,qBAC9C,KAAK,KAAK,QAAQ,KAAK,qBACV,KAAK,KAAK,QAAQ,KAAK,2BACpC,KAAK,KAAK,cAAc,KAAK,yBACZ,KAAK,KAAK,YAAY,KAAK,qBAC5C,KAAK,KAAK,QAAQ,KAAK,oBACX,KAAK,KAAK,OAAO,KAAK,mBAClC,KAAK,KAAK,MAAM,KAAK,sBACP,KAAK,KAAK,SAAS,KAAK,yBACtC,KAAK,KAAK,YAAY,KAAK,2BACR,KAAK,KAAK,cAAc,KAAK;AAAA;AAAA;AAIlD,UAAM,KAAK,IACR,2BAA2B,KAAK;AAEnC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,+BAA+B,KAAK;AAEnD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,eAAe,MAAwB;AAC3C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI;AAAA;AAAA;AAKf,UAAM,KAAK,IACR,gEAAgE,KAAK;AAGxE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,mBAAmB,MAAwB;AAC/C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI;AAAA;AAAA;AAKf,UAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,gBAAgB,MAAwB;AAC5C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI;AAAA;AAAA;AAKf,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,OAAO,MAKI;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,kDAAkD;AAAA,IACpE;AAGA,UAAM,KAAK,IAAI,0CACb,KAAK,KAAK,KAAK;AAAA;AAAA;AAMjB,UAAM,KAAK,IACR,wDAAwD,KAAK;AAGhE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAUO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,+CAA+C;AAAA,IACjE;AAGA,UAAM,KAAK,IAAI,iDACb,KAAK,eAAe,KAAK,gBACjB,KAAK,GAAG,KAAK;AAAA;AAAA;AAKvB,UAAM,KAAK,IACR,qDAAqD,KAAK;AAG7D,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,cAAc,MAUH;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAGA,UAAM,KAAK,IAAI,2DACb,KAAK,eAAe,KAAK,gBACjB,KAAK,GAAG,KAAK;AAAA;AAAA;AAKvB,UAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,sBAAsB,MAKX;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,oEACD,KAAK,gBAAgB,KAAK;AAAA;AAAA;AAM5B,UAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,qBAAqB,MAKV;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,4DACb,KAAK,SAAS,KAAK;AAAA;AAAA;AAMrB,UAAM,KAAK,IACR,sEAAsE,KAAK;AAG9E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,eAAe,MAAwB;AAC3C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI;AAAA;AAAA;AAKf,UAAM,KAAK,IACR,gEAAgE,KAAK;AAGxE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,kBAAkB,MAKL;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,4DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,gBAAgB,MAKL;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,6DACb,KAAK,eAAe,KAAK;AAAA;AAAA;AAM3B,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,sBAAsB,MAKX;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,mEACD,KAAK,eAAe,KAAK;AAAA;AAAA;AAM3B,UAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU,MAKC;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,qDAAqD;AAAA,IACvE;AAGA,UAAM,KAAK,IAAI,uDACb,KAAK,eAAe,KAAK;AAAA;AAAA;AAM3B,UAAM,KAAK,IACR,2DAA2D,KAAK;AAGnE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,8DAA8D,KAAK;AAGtE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,8DAA8D,KAAK;AAGtE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;","names":[]}