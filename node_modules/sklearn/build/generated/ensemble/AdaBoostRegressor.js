// src/generated/ensemble/AdaBoostRegressor.ts
import crypto from "node:crypto";
var AdaBoostRegressor = class {
  constructor(opts) {
    this._isInitialized = false;
    this._isDisposed = false;
    this.id = `AdaBoostRegressor${crypto.randomUUID().split("-")[0]}`;
    this.opts = opts || {};
  }
  get py() {
    return this._py;
  }
  set py(pythonBridge) {
    this._py = pythonBridge;
  }
  /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
  async init(py) {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (this._isInitialized) {
      return;
    }
    if (!py) {
      throw new Error("AdaBoostRegressor.init requires a PythonBridge instance");
    }
    this._py = py;
    await this._py.ex`
import numpy as np
from sklearn.ensemble import AdaBoostRegressor
try: bridgeAdaBoostRegressor
except NameError: bridgeAdaBoostRegressor = {}
`;
    await this._py.ex`ctor_AdaBoostRegressor = {'estimator': ${this.opts["estimator"] ?? void 0}, 'n_estimators': ${this.opts["n_estimators"] ?? void 0}, 'learning_rate': ${this.opts["learning_rate"] ?? void 0}, 'loss': ${this.opts["loss"] ?? void 0}, 'random_state': ${this.opts["random_state"] ?? void 0}, 'base_estimator': ${this.opts["base_estimator"] ?? void 0}}

ctor_AdaBoostRegressor = {k: v for k, v in ctor_AdaBoostRegressor.items() if v is not None}`;
    await this._py.ex`bridgeAdaBoostRegressor[${this.id}] = AdaBoostRegressor(**ctor_AdaBoostRegressor)`;
    this._isInitialized = true;
  }
  /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
  async dispose() {
    if (this._isDisposed) {
      return;
    }
    if (!this._isInitialized) {
      return;
    }
    await this._py.ex`del bridgeAdaBoostRegressor[${this.id}]`;
    this._isDisposed = true;
  }
  /**
    Build a boosted classifier/regressor from the training set (X, y).
   */
  async fit(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error("AdaBoostRegressor must call init() before fit()");
    }
    await this._py.ex`pms_AdaBoostRegressor_fit = {'X': np.array(${opts["X"] ?? void 0}) if ${opts["X"] !== void 0} else None, 'y': np.array(${opts["y"] ?? void 0}) if ${opts["y"] !== void 0} else None, 'sample_weight': np.array(${opts["sample_weight"] ?? void 0}) if ${opts["sample_weight"] !== void 0} else None}

pms_AdaBoostRegressor_fit = {k: v for k, v in pms_AdaBoostRegressor_fit.items() if v is not None}`;
    await this._py.ex`res_AdaBoostRegressor_fit = bridgeAdaBoostRegressor[${this.id}].fit(**pms_AdaBoostRegressor_fit)`;
    return this._py`res_AdaBoostRegressor_fit.tolist() if hasattr(res_AdaBoostRegressor_fit, 'tolist') else res_AdaBoostRegressor_fit`;
  }
  /**
      Get metadata routing of this object.
  
      Please check [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.
     */
  async get_metadata_routing(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "AdaBoostRegressor must call init() before get_metadata_routing()"
      );
    }
    await this._py.ex`pms_AdaBoostRegressor_get_metadata_routing = {'routing': ${opts["routing"] ?? void 0}}

pms_AdaBoostRegressor_get_metadata_routing = {k: v for k, v in pms_AdaBoostRegressor_get_metadata_routing.items() if v is not None}`;
    await this._py.ex`res_AdaBoostRegressor_get_metadata_routing = bridgeAdaBoostRegressor[${this.id}].get_metadata_routing(**pms_AdaBoostRegressor_get_metadata_routing)`;
    return this._py`res_AdaBoostRegressor_get_metadata_routing.tolist() if hasattr(res_AdaBoostRegressor_get_metadata_routing, 'tolist') else res_AdaBoostRegressor_get_metadata_routing`;
  }
  /**
      Predict regression value for X.
  
      The predicted regression value of an input sample is computed as the weighted median prediction of the regressors in the ensemble.
     */
  async predict(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error("AdaBoostRegressor must call init() before predict()");
    }
    await this._py.ex`pms_AdaBoostRegressor_predict = {'X': np.array(${opts["X"] ?? void 0}) if ${opts["X"] !== void 0} else None}

pms_AdaBoostRegressor_predict = {k: v for k, v in pms_AdaBoostRegressor_predict.items() if v is not None}`;
    await this._py.ex`res_AdaBoostRegressor_predict = bridgeAdaBoostRegressor[${this.id}].predict(**pms_AdaBoostRegressor_predict)`;
    return this._py`res_AdaBoostRegressor_predict.tolist() if hasattr(res_AdaBoostRegressor_predict, 'tolist') else res_AdaBoostRegressor_predict`;
  }
  /**
      Return the coefficient of determination of the prediction.
  
      The coefficient of determination \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares `((y\_true \- y\_pred)\*\* 2).sum()` and \\(v\\) is the total sum of squares `((y\_true \- y\_true.mean()) \*\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\(R^2\\) score of 0.0.
     */
  async score(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error("AdaBoostRegressor must call init() before score()");
    }
    await this._py.ex`pms_AdaBoostRegressor_score = {'X': np.array(${opts["X"] ?? void 0}) if ${opts["X"] !== void 0} else None, 'y': np.array(${opts["y"] ?? void 0}) if ${opts["y"] !== void 0} else None, 'sample_weight': np.array(${opts["sample_weight"] ?? void 0}) if ${opts["sample_weight"] !== void 0} else None}

pms_AdaBoostRegressor_score = {k: v for k, v in pms_AdaBoostRegressor_score.items() if v is not None}`;
    await this._py.ex`res_AdaBoostRegressor_score = bridgeAdaBoostRegressor[${this.id}].score(**pms_AdaBoostRegressor_score)`;
    return this._py`res_AdaBoostRegressor_score.tolist() if hasattr(res_AdaBoostRegressor_score, 'tolist') else res_AdaBoostRegressor_score`;
  }
  /**
      Request metadata passed to the `fit` method.
  
      Note that this method is only relevant if `enable\_metadata\_routing=True` (see [`sklearn.set\_config`](sklearn.set_config.html#sklearn.set_config "sklearn.set_config")). Please see [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.
  
      The options for each parameter are:
     */
  async set_fit_request(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "AdaBoostRegressor must call init() before set_fit_request()"
      );
    }
    await this._py.ex`pms_AdaBoostRegressor_set_fit_request = {'sample_weight': ${opts["sample_weight"] ?? void 0}}

pms_AdaBoostRegressor_set_fit_request = {k: v for k, v in pms_AdaBoostRegressor_set_fit_request.items() if v is not None}`;
    await this._py.ex`res_AdaBoostRegressor_set_fit_request = bridgeAdaBoostRegressor[${this.id}].set_fit_request(**pms_AdaBoostRegressor_set_fit_request)`;
    return this._py`res_AdaBoostRegressor_set_fit_request.tolist() if hasattr(res_AdaBoostRegressor_set_fit_request, 'tolist') else res_AdaBoostRegressor_set_fit_request`;
  }
  /**
      Request metadata passed to the `score` method.
  
      Note that this method is only relevant if `enable\_metadata\_routing=True` (see [`sklearn.set\_config`](sklearn.set_config.html#sklearn.set_config "sklearn.set_config")). Please see [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.
  
      The options for each parameter are:
     */
  async set_score_request(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "AdaBoostRegressor must call init() before set_score_request()"
      );
    }
    await this._py.ex`pms_AdaBoostRegressor_set_score_request = {'sample_weight': ${opts["sample_weight"] ?? void 0}}

pms_AdaBoostRegressor_set_score_request = {k: v for k, v in pms_AdaBoostRegressor_set_score_request.items() if v is not None}`;
    await this._py.ex`res_AdaBoostRegressor_set_score_request = bridgeAdaBoostRegressor[${this.id}].set_score_request(**pms_AdaBoostRegressor_set_score_request)`;
    return this._py`res_AdaBoostRegressor_set_score_request.tolist() if hasattr(res_AdaBoostRegressor_set_score_request, 'tolist') else res_AdaBoostRegressor_set_score_request`;
  }
  /**
      Return staged predictions for X.
  
      The predicted regression value of an input sample is computed as the weighted median prediction of the regressors in the ensemble.
  
      This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost.
     */
  async staged_predict(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "AdaBoostRegressor must call init() before staged_predict()"
      );
    }
    await this._py.ex`pms_AdaBoostRegressor_staged_predict = {'X': np.array(${opts["X"] ?? void 0}) if ${opts["X"] !== void 0} else None}

pms_AdaBoostRegressor_staged_predict = {k: v for k, v in pms_AdaBoostRegressor_staged_predict.items() if v is not None}`;
    await this._py.ex`res_AdaBoostRegressor_staged_predict = bridgeAdaBoostRegressor[${this.id}].staged_predict(**pms_AdaBoostRegressor_staged_predict)`;
    return this._py`res_AdaBoostRegressor_staged_predict.tolist() if hasattr(res_AdaBoostRegressor_staged_predict, 'tolist') else res_AdaBoostRegressor_staged_predict`;
  }
  /**
      Return staged scores for X, y.
  
      This generator method yields the ensemble score after each iteration of boosting and therefore allows monitoring, such as to determine the score on a test set after each boost.
     */
  async staged_score(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "AdaBoostRegressor must call init() before staged_score()"
      );
    }
    await this._py.ex`pms_AdaBoostRegressor_staged_score = {'X': np.array(${opts["X"] ?? void 0}) if ${opts["X"] !== void 0} else None, 'y': np.array(${opts["y"] ?? void 0}) if ${opts["y"] !== void 0} else None, 'sample_weight': np.array(${opts["sample_weight"] ?? void 0}) if ${opts["sample_weight"] !== void 0} else None}

pms_AdaBoostRegressor_staged_score = {k: v for k, v in pms_AdaBoostRegressor_staged_score.items() if v is not None}`;
    await this._py.ex`res_AdaBoostRegressor_staged_score = bridgeAdaBoostRegressor[${this.id}].staged_score(**pms_AdaBoostRegressor_staged_score)`;
    return this._py`res_AdaBoostRegressor_staged_score.tolist() if hasattr(res_AdaBoostRegressor_staged_score, 'tolist') else res_AdaBoostRegressor_staged_score`;
  }
  /**
    The base estimator from which the ensemble is grown.
   */
  get estimator_() {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "AdaBoostRegressor must call init() before accessing estimator_"
      );
    }
    return (async () => {
      await this._py.ex`attr_AdaBoostRegressor_estimator_ = bridgeAdaBoostRegressor[${this.id}].estimator_`;
      return this._py`attr_AdaBoostRegressor_estimator_.tolist() if hasattr(attr_AdaBoostRegressor_estimator_, 'tolist') else attr_AdaBoostRegressor_estimator_`;
    })();
  }
  /**
    The collection of fitted sub-estimators.
   */
  get estimators_() {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "AdaBoostRegressor must call init() before accessing estimators_"
      );
    }
    return (async () => {
      await this._py.ex`attr_AdaBoostRegressor_estimators_ = bridgeAdaBoostRegressor[${this.id}].estimators_`;
      return this._py`attr_AdaBoostRegressor_estimators_.tolist() if hasattr(attr_AdaBoostRegressor_estimators_, 'tolist') else attr_AdaBoostRegressor_estimators_`;
    })();
  }
  /**
    Weights for each estimator in the boosted ensemble.
   */
  get estimator_weights_() {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "AdaBoostRegressor must call init() before accessing estimator_weights_"
      );
    }
    return (async () => {
      await this._py.ex`attr_AdaBoostRegressor_estimator_weights_ = bridgeAdaBoostRegressor[${this.id}].estimator_weights_`;
      return this._py`attr_AdaBoostRegressor_estimator_weights_.tolist() if hasattr(attr_AdaBoostRegressor_estimator_weights_, 'tolist') else attr_AdaBoostRegressor_estimator_weights_`;
    })();
  }
  /**
    Regression error for each estimator in the boosted ensemble.
   */
  get estimator_errors_() {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "AdaBoostRegressor must call init() before accessing estimator_errors_"
      );
    }
    return (async () => {
      await this._py.ex`attr_AdaBoostRegressor_estimator_errors_ = bridgeAdaBoostRegressor[${this.id}].estimator_errors_`;
      return this._py`attr_AdaBoostRegressor_estimator_errors_.tolist() if hasattr(attr_AdaBoostRegressor_estimator_errors_, 'tolist') else attr_AdaBoostRegressor_estimator_errors_`;
    })();
  }
  /**
    Number of features seen during [fit](../../glossary.html#term-fit).
   */
  get n_features_in_() {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "AdaBoostRegressor must call init() before accessing n_features_in_"
      );
    }
    return (async () => {
      await this._py.ex`attr_AdaBoostRegressor_n_features_in_ = bridgeAdaBoostRegressor[${this.id}].n_features_in_`;
      return this._py`attr_AdaBoostRegressor_n_features_in_.tolist() if hasattr(attr_AdaBoostRegressor_n_features_in_, 'tolist') else attr_AdaBoostRegressor_n_features_in_`;
    })();
  }
  /**
    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
   */
  get feature_names_in_() {
    if (this._isDisposed) {
      throw new Error(
        "This AdaBoostRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "AdaBoostRegressor must call init() before accessing feature_names_in_"
      );
    }
    return (async () => {
      await this._py.ex`attr_AdaBoostRegressor_feature_names_in_ = bridgeAdaBoostRegressor[${this.id}].feature_names_in_`;
      return this._py`attr_AdaBoostRegressor_feature_names_in_.tolist() if hasattr(attr_AdaBoostRegressor_feature_names_in_, 'tolist') else attr_AdaBoostRegressor_feature_names_in_`;
    })();
  }
};
export {
  AdaBoostRegressor
};
//# sourceMappingURL=AdaBoostRegressor.js.map