// src/generated/ensemble/HistGradientBoostingRegressor.ts
import crypto from "node:crypto";
var HistGradientBoostingRegressor = class {
  constructor(opts) {
    this._isInitialized = false;
    this._isDisposed = false;
    this.id = `HistGradientBoostingRegressor${crypto.randomUUID().split("-")[0]}`;
    this.opts = opts || {};
  }
  get py() {
    return this._py;
  }
  set py(pythonBridge) {
    this._py = pythonBridge;
  }
  /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
  async init(py) {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (this._isInitialized) {
      return;
    }
    if (!py) {
      throw new Error(
        "HistGradientBoostingRegressor.init requires a PythonBridge instance"
      );
    }
    this._py = py;
    await this._py.ex`
import numpy as np
from sklearn.ensemble import HistGradientBoostingRegressor
try: bridgeHistGradientBoostingRegressor
except NameError: bridgeHistGradientBoostingRegressor = {}
`;
    await this._py.ex`ctor_HistGradientBoostingRegressor = {'loss': ${this.opts["loss"] ?? void 0}, 'quantile': ${this.opts["quantile"] ?? void 0}, 'learning_rate': ${this.opts["learning_rate"] ?? void 0}, 'max_iter': ${this.opts["max_iter"] ?? void 0}, 'max_leaf_nodes': ${this.opts["max_leaf_nodes"] ?? void 0}, 'max_depth': ${this.opts["max_depth"] ?? void 0}, 'min_samples_leaf': ${this.opts["min_samples_leaf"] ?? void 0}, 'l2_regularization': ${this.opts["l2_regularization"] ?? void 0}, 'max_bins': ${this.opts["max_bins"] ?? void 0}, 'categorical_features': np.array(${this.opts["categorical_features"] ?? void 0}) if ${this.opts["categorical_features"] !== void 0} else None, 'monotonic_cst': np.array(${this.opts["monotonic_cst"] ?? void 0}) if ${this.opts["monotonic_cst"] !== void 0} else None, 'interaction_cst': ${this.opts["interaction_cst"] ?? void 0}, 'warm_start': ${this.opts["warm_start"] ?? void 0}, 'early_stopping': ${this.opts["early_stopping"] ?? void 0}, 'scoring': ${this.opts["scoring"] ?? void 0}, 'validation_fraction': ${this.opts["validation_fraction"] ?? void 0}, 'n_iter_no_change': ${this.opts["n_iter_no_change"] ?? void 0}, 'tol': ${this.opts["tol"] ?? void 0}, 'verbose': ${this.opts["verbose"] ?? void 0}, 'random_state': ${this.opts["random_state"] ?? void 0}}

ctor_HistGradientBoostingRegressor = {k: v for k, v in ctor_HistGradientBoostingRegressor.items() if v is not None}`;
    await this._py.ex`bridgeHistGradientBoostingRegressor[${this.id}] = HistGradientBoostingRegressor(**ctor_HistGradientBoostingRegressor)`;
    this._isInitialized = true;
  }
  /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
  async dispose() {
    if (this._isDisposed) {
      return;
    }
    if (!this._isInitialized) {
      return;
    }
    await this._py.ex`del bridgeHistGradientBoostingRegressor[${this.id}]`;
    this._isDisposed = true;
  }
  /**
    Fit the gradient boosting model.
   */
  async fit(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before fit()"
      );
    }
    await this._py.ex`pms_HistGradientBoostingRegressor_fit = {'X': np.array(${opts["X"] ?? void 0}) if ${opts["X"] !== void 0} else None, 'y': np.array(${opts["y"] ?? void 0}) if ${opts["y"] !== void 0} else None, 'sample_weight': np.array(${opts["sample_weight"] ?? void 0}) if ${opts["sample_weight"] !== void 0} else None}

pms_HistGradientBoostingRegressor_fit = {k: v for k, v in pms_HistGradientBoostingRegressor_fit.items() if v is not None}`;
    await this._py.ex`res_HistGradientBoostingRegressor_fit = bridgeHistGradientBoostingRegressor[${this.id}].fit(**pms_HistGradientBoostingRegressor_fit)`;
    return this._py`res_HistGradientBoostingRegressor_fit.tolist() if hasattr(res_HistGradientBoostingRegressor_fit, 'tolist') else res_HistGradientBoostingRegressor_fit`;
  }
  /**
      Get metadata routing of this object.
  
      Please check [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.
     */
  async get_metadata_routing(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before get_metadata_routing()"
      );
    }
    await this._py.ex`pms_HistGradientBoostingRegressor_get_metadata_routing = {'routing': ${opts["routing"] ?? void 0}}

pms_HistGradientBoostingRegressor_get_metadata_routing = {k: v for k, v in pms_HistGradientBoostingRegressor_get_metadata_routing.items() if v is not None}`;
    await this._py.ex`res_HistGradientBoostingRegressor_get_metadata_routing = bridgeHistGradientBoostingRegressor[${this.id}].get_metadata_routing(**pms_HistGradientBoostingRegressor_get_metadata_routing)`;
    return this._py`res_HistGradientBoostingRegressor_get_metadata_routing.tolist() if hasattr(res_HistGradientBoostingRegressor_get_metadata_routing, 'tolist') else res_HistGradientBoostingRegressor_get_metadata_routing`;
  }
  /**
    Predict values for X.
   */
  async predict(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before predict()"
      );
    }
    await this._py.ex`pms_HistGradientBoostingRegressor_predict = {'X': ${opts["X"] ?? void 0}}

pms_HistGradientBoostingRegressor_predict = {k: v for k, v in pms_HistGradientBoostingRegressor_predict.items() if v is not None}`;
    await this._py.ex`res_HistGradientBoostingRegressor_predict = bridgeHistGradientBoostingRegressor[${this.id}].predict(**pms_HistGradientBoostingRegressor_predict)`;
    return this._py`res_HistGradientBoostingRegressor_predict.tolist() if hasattr(res_HistGradientBoostingRegressor_predict, 'tolist') else res_HistGradientBoostingRegressor_predict`;
  }
  /**
      Return the coefficient of determination of the prediction.
  
      The coefficient of determination \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares `((y\_true \- y\_pred)\*\* 2).sum()` and \\(v\\) is the total sum of squares `((y\_true \- y\_true.mean()) \*\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\(R^2\\) score of 0.0.
     */
  async score(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before score()"
      );
    }
    await this._py.ex`pms_HistGradientBoostingRegressor_score = {'X': np.array(${opts["X"] ?? void 0}) if ${opts["X"] !== void 0} else None, 'y': np.array(${opts["y"] ?? void 0}) if ${opts["y"] !== void 0} else None, 'sample_weight': np.array(${opts["sample_weight"] ?? void 0}) if ${opts["sample_weight"] !== void 0} else None}

pms_HistGradientBoostingRegressor_score = {k: v for k, v in pms_HistGradientBoostingRegressor_score.items() if v is not None}`;
    await this._py.ex`res_HistGradientBoostingRegressor_score = bridgeHistGradientBoostingRegressor[${this.id}].score(**pms_HistGradientBoostingRegressor_score)`;
    return this._py`res_HistGradientBoostingRegressor_score.tolist() if hasattr(res_HistGradientBoostingRegressor_score, 'tolist') else res_HistGradientBoostingRegressor_score`;
  }
  /**
      Request metadata passed to the `fit` method.
  
      Note that this method is only relevant if `enable\_metadata\_routing=True` (see [`sklearn.set\_config`](sklearn.set_config.html#sklearn.set_config "sklearn.set_config")). Please see [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.
  
      The options for each parameter are:
     */
  async set_fit_request(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before set_fit_request()"
      );
    }
    await this._py.ex`pms_HistGradientBoostingRegressor_set_fit_request = {'sample_weight': ${opts["sample_weight"] ?? void 0}}

pms_HistGradientBoostingRegressor_set_fit_request = {k: v for k, v in pms_HistGradientBoostingRegressor_set_fit_request.items() if v is not None}`;
    await this._py.ex`res_HistGradientBoostingRegressor_set_fit_request = bridgeHistGradientBoostingRegressor[${this.id}].set_fit_request(**pms_HistGradientBoostingRegressor_set_fit_request)`;
    return this._py`res_HistGradientBoostingRegressor_set_fit_request.tolist() if hasattr(res_HistGradientBoostingRegressor_set_fit_request, 'tolist') else res_HistGradientBoostingRegressor_set_fit_request`;
  }
  /**
      Request metadata passed to the `score` method.
  
      Note that this method is only relevant if `enable\_metadata\_routing=True` (see [`sklearn.set\_config`](sklearn.set_config.html#sklearn.set_config "sklearn.set_config")). Please see [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.
  
      The options for each parameter are:
     */
  async set_score_request(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before set_score_request()"
      );
    }
    await this._py.ex`pms_HistGradientBoostingRegressor_set_score_request = {'sample_weight': ${opts["sample_weight"] ?? void 0}}

pms_HistGradientBoostingRegressor_set_score_request = {k: v for k, v in pms_HistGradientBoostingRegressor_set_score_request.items() if v is not None}`;
    await this._py.ex`res_HistGradientBoostingRegressor_set_score_request = bridgeHistGradientBoostingRegressor[${this.id}].set_score_request(**pms_HistGradientBoostingRegressor_set_score_request)`;
    return this._py`res_HistGradientBoostingRegressor_set_score_request.tolist() if hasattr(res_HistGradientBoostingRegressor_set_score_request, 'tolist') else res_HistGradientBoostingRegressor_set_score_request`;
  }
  /**
      Predict regression target for each iteration.
  
      This method allows monitoring (i.e. determine error on testing set) after each stage.
     */
  async staged_predict(opts) {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before staged_predict()"
      );
    }
    await this._py.ex`pms_HistGradientBoostingRegressor_staged_predict = {'X': np.array(${opts["X"] ?? void 0}) if ${opts["X"] !== void 0} else None}

pms_HistGradientBoostingRegressor_staged_predict = {k: v for k, v in pms_HistGradientBoostingRegressor_staged_predict.items() if v is not None}`;
    await this._py.ex`res_HistGradientBoostingRegressor_staged_predict = bridgeHistGradientBoostingRegressor[${this.id}].staged_predict(**pms_HistGradientBoostingRegressor_staged_predict)`;
    return this._py`res_HistGradientBoostingRegressor_staged_predict.tolist() if hasattr(res_HistGradientBoostingRegressor_staged_predict, 'tolist') else res_HistGradientBoostingRegressor_staged_predict`;
  }
  /**
    Indicates whether early stopping is used during training.
   */
  get do_early_stopping_() {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before accessing do_early_stopping_"
      );
    }
    return (async () => {
      await this._py.ex`attr_HistGradientBoostingRegressor_do_early_stopping_ = bridgeHistGradientBoostingRegressor[${this.id}].do_early_stopping_`;
      return this._py`attr_HistGradientBoostingRegressor_do_early_stopping_.tolist() if hasattr(attr_HistGradientBoostingRegressor_do_early_stopping_, 'tolist') else attr_HistGradientBoostingRegressor_do_early_stopping_`;
    })();
  }
  /**
    The number of tree that are built at each iteration. For regressors, this is always 1.
   */
  get n_trees_per_iteration_() {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before accessing n_trees_per_iteration_"
      );
    }
    return (async () => {
      await this._py.ex`attr_HistGradientBoostingRegressor_n_trees_per_iteration_ = bridgeHistGradientBoostingRegressor[${this.id}].n_trees_per_iteration_`;
      return this._py`attr_HistGradientBoostingRegressor_n_trees_per_iteration_.tolist() if hasattr(attr_HistGradientBoostingRegressor_n_trees_per_iteration_, 'tolist') else attr_HistGradientBoostingRegressor_n_trees_per_iteration_`;
    })();
  }
  /**
    The scores at each iteration on the training data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the `scoring` parameter. If `scoring` is not ‘loss’, scores are computed on a subset of at most 10 000 samples. Empty if no early stopping.
   */
  get train_score_() {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before accessing train_score_"
      );
    }
    return (async () => {
      await this._py.ex`attr_HistGradientBoostingRegressor_train_score_ = bridgeHistGradientBoostingRegressor[${this.id}].train_score_`;
      return this._py`attr_HistGradientBoostingRegressor_train_score_.tolist() if hasattr(attr_HistGradientBoostingRegressor_train_score_, 'tolist') else attr_HistGradientBoostingRegressor_train_score_`;
    })();
  }
  /**
    The scores at each iteration on the held-out validation data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the `scoring` parameter. Empty if no early stopping or if `validation\_fraction` is `undefined`.
   */
  get validation_score_() {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before accessing validation_score_"
      );
    }
    return (async () => {
      await this._py.ex`attr_HistGradientBoostingRegressor_validation_score_ = bridgeHistGradientBoostingRegressor[${this.id}].validation_score_`;
      return this._py`attr_HistGradientBoostingRegressor_validation_score_.tolist() if hasattr(attr_HistGradientBoostingRegressor_validation_score_, 'tolist') else attr_HistGradientBoostingRegressor_validation_score_`;
    })();
  }
  /**
    Boolean mask for the categorical features. `undefined` if there are no categorical features.
   */
  get is_categorical_() {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before accessing is_categorical_"
      );
    }
    return (async () => {
      await this._py.ex`attr_HistGradientBoostingRegressor_is_categorical_ = bridgeHistGradientBoostingRegressor[${this.id}].is_categorical_`;
      return this._py`attr_HistGradientBoostingRegressor_is_categorical_.tolist() if hasattr(attr_HistGradientBoostingRegressor_is_categorical_, 'tolist') else attr_HistGradientBoostingRegressor_is_categorical_`;
    })();
  }
  /**
    Number of features seen during [fit](../../glossary.html#term-fit).
   */
  get n_features_in_() {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before accessing n_features_in_"
      );
    }
    return (async () => {
      await this._py.ex`attr_HistGradientBoostingRegressor_n_features_in_ = bridgeHistGradientBoostingRegressor[${this.id}].n_features_in_`;
      return this._py`attr_HistGradientBoostingRegressor_n_features_in_.tolist() if hasattr(attr_HistGradientBoostingRegressor_n_features_in_, 'tolist') else attr_HistGradientBoostingRegressor_n_features_in_`;
    })();
  }
  /**
    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
   */
  get feature_names_in_() {
    if (this._isDisposed) {
      throw new Error(
        "This HistGradientBoostingRegressor instance has already been disposed"
      );
    }
    if (!this._isInitialized) {
      throw new Error(
        "HistGradientBoostingRegressor must call init() before accessing feature_names_in_"
      );
    }
    return (async () => {
      await this._py.ex`attr_HistGradientBoostingRegressor_feature_names_in_ = bridgeHistGradientBoostingRegressor[${this.id}].feature_names_in_`;
      return this._py`attr_HistGradientBoostingRegressor_feature_names_in_.tolist() if hasattr(attr_HistGradientBoostingRegressor_feature_names_in_, 'tolist') else attr_HistGradientBoostingRegressor_feature_names_in_`;
    })();
  }
};
export {
  HistGradientBoostingRegressor
};
//# sourceMappingURL=HistGradientBoostingRegressor.js.map