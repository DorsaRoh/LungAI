import { PythonBridge, NDArray, ArrayLike } from '@/sklearn/types';
/**
  Sparse inverse covariance estimation with an l1-penalized estimator.

  Read more in the [User Guide](../covariance.html#sparse-inverse-covariance).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLasso.html)
 */
export declare class GraphicalLasso {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance. Range is (0, inf\].
    
          @defaultValue `0.01`
         */
        alpha?: number;
        /**
          The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p > n. Elsewhere prefer cd which is more numerically stable.
    
          @defaultValue `'cd'`
         */
        mode?: 'cd' | 'lars';
        /**
          If covariance is “precomputed”, the input data in `fit` is assumed to be the covariance matrix. If `undefined`, the empirical covariance is estimated from the data `X`.
         */
        covariance?: 'precomputed';
        /**
          The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. Range is (0, inf\].
    
          @defaultValue `0.0001`
         */
        tol?: number;
        /**
          The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=’cd’. Range is (0, inf\].
    
          @defaultValue `0.0001`
         */
        enet_tol?: number;
        /**
          The maximum number of iterations.
    
          @defaultValue `100`
         */
        max_iter?: number;
        /**
          If verbose is `true`, the objective function and dual gap are plotted at each iteration.
    
          @defaultValue `false`
         */
        verbose?: boolean;
        /**
          The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Default is `np.finfo(np.float64).eps`.
         */
        eps?: number;
        /**
          If `true`, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If `false`, data are centered before computation.
    
          @defaultValue `false`
         */
        assume_centered?: boolean;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Compute the Mean Squared Error between two covariance estimators.
     */
    error_norm(opts: {
        /**
          The covariance to compare with.
         */
        comp_cov?: ArrayLike[];
        /**
          The type of norm used to compute the error. Available error types: - ‘frobenius’ (default): sqrt(tr(A^t.A)) - ‘spectral’: sqrt(max(eigenvalues(A^t.A)) where A is the error `(comp\_cov \- self.covariance\_)`.
    
          @defaultValue `'frobenius'`
         */
        norm?: 'frobenius' | 'spectral';
        /**
          If `true` (default), the squared error norm is divided by n\_features. If `false`, the squared error norm is not rescaled.
    
          @defaultValue `true`
         */
        scaling?: boolean;
        /**
          Whether to compute the squared error norm or the error norm. If `true` (default), the squared error norm is returned. If `false`, the error norm is returned.
    
          @defaultValue `true`
         */
        squared?: boolean;
    }): Promise<number>;
    /**
      Fit the GraphicalLasso model to X.
     */
    fit(opts: {
        /**
          Data from which to compute the covariance estimate.
         */
        X?: ArrayLike[];
        /**
          Not used, present for API consistency by convention.
         */
        y?: any;
    }): Promise<any>;
    /**
      Get metadata routing of this object.
  
      Please check [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.
     */
    get_metadata_routing(opts: {
        /**
          A [`MetadataRequest`](sklearn.utils.metadata_routing.MetadataRequest.html#sklearn.utils.metadata_routing.MetadataRequest "sklearn.utils.metadata_routing.MetadataRequest") encapsulating routing information.
         */
        routing?: any;
    }): Promise<any>;
    /**
      Getter for the precision matrix.
     */
    get_precision(opts: {
        /**
          The precision matrix associated to the current covariance object.
         */
        precision_?: ArrayLike[];
    }): Promise<any>;
    /**
      Compute the squared Mahalanobis distances of given observations.
     */
    mahalanobis(opts: {
        /**
          The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.
         */
        X?: ArrayLike[];
    }): Promise<NDArray>;
    /**
      Compute the log-likelihood of `X\_test` under the estimated Gaussian model.
  
      The Gaussian model is defined by its mean and covariance matrix which are represented respectively by `self.location\_` and `self.covariance\_`.
     */
    score(opts: {
        /**
          Test data of which we compute the likelihood, where `n\_samples` is the number of samples and `n\_features` is the number of features. `X\_test` is assumed to be drawn from the same distribution than the data used in fit (including centering).
         */
        X_test?: ArrayLike[];
        /**
          Not used, present for API consistency by convention.
         */
        y?: any;
    }): Promise<number>;
    /**
      Request metadata passed to the `score` method.
  
      Note that this method is only relevant if `enable\_metadata\_routing=True` (see [`sklearn.set\_config`](sklearn.set_config.html#sklearn.set_config "sklearn.set_config")). Please see [User Guide](../../metadata_routing.html#metadata-routing) on how the routing mechanism works.
  
      The options for each parameter are:
     */
    set_score_request(opts: {
        /**
          Metadata routing for `X\_test` parameter in `score`.
         */
        X_test?: string | boolean;
    }): Promise<any>;
    /**
      Estimated location, i.e. the estimated mean.
     */
    get location_(): Promise<NDArray>;
    /**
      Estimated covariance matrix
     */
    get covariance_(): Promise<NDArray[]>;
    /**
      Estimated pseudo inverse matrix.
     */
    get precision_(): Promise<NDArray[]>;
    /**
      Number of iterations run.
     */
    get n_iter_(): Promise<number>;
    /**
      The list of values of the objective function and the dual gap at each iteration. Returned only if return\_costs is `true`.
     */
    get costs_(): Promise<any>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
//# sourceMappingURL=GraphicalLasso.d.ts.map